{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUWCAY5opP87"
   },
   "source": [
    "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\"></p>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEkVD5qHpP89"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wj5MrpmRpP89"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Элементы теории оптимизации. Производные и частные производные.</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1xIGzO5pP8-"
   },
   "source": [
    "<p style=\"text-align: center;\">(На основе https://github.com/romasoletskyi/Machine-Learning-Course)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKzSbzJQ90GT"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODaZDX75pP8-"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Приращение линейной функции</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tldAx431pP8_"
   },
   "source": [
    "Давайте рассмотрим линейную функцию $y=kx+b$ и построим график: <br>  \n",
    "\n",
    "![source: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/c/c1/Wiki_slope_in_2d.svg) <br>  \n",
    "\n",
    "Введём понятие **приращения** функции в точке $(x, y)$ как отношение вертикального изменения (измненеия функции по вертикали) $\\Delta y$ к горизонтальному изменению $\\Delta x$ и вычислим приращение для линейной функции:  \n",
    "\n",
    "$$приращение (\"slope\")=\\frac{\\Delta y}{\\Delta x}=\\frac{y_2-y_1}{x_2-x_1}=\\frac{kx_2+b-kx_1-b}{x_2-x_1}=k\\frac{x_2-x_1}{x_2-x_1}=k$$  \n",
    "\n",
    "Видим, что приращение в точке у прямой не зависит от $x$ и $\\Delta x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObcFoC9JpP9A"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Приращение произвольной функции</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNfWm09WpP9A"
   },
   "source": [
    "Но что, если функция не линейная, а произвольная $f(x)$?  \n",
    "В таком случае просто нарисуем **касательную ** в точке, в которой ищем приращение, и будем смотреть уже на приращение касательной. Так как касательная - это прямая, мы уже знаем, какое у неё приращение (см. выше).\n",
    "![source: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/d/d2/Tangent-calculus.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "137KHuBjpP9B"
   },
   "source": [
    "Имея график функции мы, конечно, можем нарисовать касательную в точке. Но часто функции заданы аналитически, и хочется уметь сразу быстро получать формулу для приращения функциии в точке. Тут на помощь приходит **производная**.  Давайте посмотрим на определение производной его с нашим понятием приращения:  \n",
    "\n",
    "$$f'(x) = \\lim_{\\Delta x \\to 0}\\frac{\\Delta y}{\\Delta x} = \\lim_{\\Delta x \\to 0}\\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$$  \n",
    "\n",
    "То есть по сути, значение производной функции в точке - это и есть приращение функции, если мы стремим длину отрезка $\\Delta x$ к нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YksIkmlpP9C"
   },
   "source": [
    "Посомтрим на интерактивное демо, демонстрирующее стремление $\\Delta x$ к нулю (*в Google Colab работать не будет!*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "v9rhGojJpP9D",
    "outputId": "3535c707-7540-47c8-b823-fa66cafe38cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /home/runfme/anaconda3/lib/python3.6/site-packages (7.0.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipywidgets) (4.6.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipywidgets) (4.3.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.0.0 in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipywidgets) (3.0.2)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipywidgets) (6.1.0)\n",
      "Requirement already satisfied: jupyter_client in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: tornado>=4.0 in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets) (4.5.2)\n",
      "Requirement already satisfied: ipython_genutils in /home/runfme/anaconda3/lib/python3.6/site-packages (from traitlets>=4.3.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: six in /home/runfme/anaconda3/lib/python3.6/site-packages (from traitlets>=4.3.1->ipywidgets) (1.11.0)\n",
      "Requirement already satisfied: decorator in /home/runfme/anaconda3/lib/python3.6/site-packages (from traitlets>=4.3.1->ipywidgets) (4.1.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/runfme/anaconda3/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets) (2.6.0)\n",
      "Requirement already satisfied: jupyter_core in /home/runfme/anaconda3/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets) (4.3.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/runfme/anaconda3/lib/python3.6/site-packages (from widgetsnbextension~=3.0.0->ipywidgets) (5.0.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (36.5.0.post20170921)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.10.2)\n",
      "Requirement already satisfied: pickleshare in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.4)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.8.1)\n",
      "Requirement already satisfied: prompt_toolkit<2.0.0,>=1.0.4 in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (1.0.15)\n",
      "Requirement already satisfied: pygments in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: pexpect in /home/runfme/anaconda3/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (4.2.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /home/runfme/anaconda3/lib/python3.6/site-packages (from jupyter_client->ipykernel>=4.5.1->ipywidgets) (16.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/runfme/anaconda3/lib/python3.6/site-packages (from jupyter_client->ipykernel>=4.5.1->ipywidgets) (2.6.1)\n",
      "Requirement already satisfied: wcwidth in /home/runfme/anaconda3/lib/python3.6/site-packages (from prompt_toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (0.1.7)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install ipywidgets\n",
    "from __future__ import print_function\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJ_xbrHXpP9I"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc58c10de87f449ca3da5c03930738b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lg_z=(-0.5,4.0,0.1))\n",
    "def f(lg_z=1.0):\n",
    "    z = 10 ** lg_z\n",
    "    x_min = 1.5 - 6/z\n",
    "    x_max = 1.5 + 6/z\n",
    "    l_min = 1.5 - 4/z\n",
    "    l_max = 1.5 + 4/z\n",
    "    xstep = (x_max - x_min)/100\n",
    "    lstep = (l_max - l_min)/100\n",
    "    \n",
    "    x = np.arange(x_min, x_max, xstep)\n",
    "    \n",
    "    plt.plot(x, np.sin(x), '-b')     \n",
    "    \n",
    "    plt.plot((l_min,l_max), (np.sin(l_min), np.sin(l_max)), '-r')\n",
    "    plt.plot((l_min,l_max), (np.sin(l_min), np.sin(l_min)), '-r')\n",
    "    plt.plot((l_max,l_max), (np.sin(l_min), np.sin(l_max)), '-r')\n",
    "    \n",
    "    yax = plt.ylim()    \n",
    "    \n",
    "    plt.text(l_max + 0.1/z, (np.sin(l_min) + np.sin(l_max)) / 2, \"$\\Delta y$\")\n",
    "    plt.text((l_min + l_max)/2, np.sin(l_min) - (yax[1]-yax[0]) / 20, \"$\\Delta x$\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print('slope =', (np.sin(l_max) - np.sin(l_min)) / (l_max - l_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8CYa2CRpP9N"
   },
   "source": [
    "Видим, что при уменьшении отрезка $\\Delta x$, значение приращения стабилизируется (перестаёт изменяться). Это число и есть приращение функции в точке, равное проиводной функции в точке. Производную функции $f(x)$ в точке x обознают как $f'(x)$ или как $\\frac{d}{dx}(f(x))$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMwBqnhVpP9N"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Пример вычисления проиводной</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JwlAAsznpP9P"
   },
   "source": [
    "Возьмём производную по определению:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6R_rnMsqpP9P"
   },
   "source": [
    "1. $f(x)=x$  \n",
    "\n",
    "$$\\frac{\\Delta y}{\\Delta x}=\\frac{x+\\Delta x-x}{\\Delta x}=1\\Rightarrow \\mathbf{\\frac{d}{dx}(x)=1}$$  \n",
    "\n",
    "2. $f(x)=x^2$  \n",
    "\n",
    "$$\\frac{\\Delta y}{\\Delta x}=\\frac{(x+\\Delta x)^2-x^2}{\\Delta x}=\\frac{x^2+2x\\Delta x+\\Delta x^2-x^2}{\\Delta x}=2x+\\Delta x\\rightarrow 2x (\\Delta x\\rightarrow 0)\\Rightarrow \\mathbf{\\frac{d}{dx}(x^2)=2x}$$  \n",
    "    \n",
    "3. В общем случае для степенной функции $f(x)=x^n$ формула будет такой:  \n",
    "\n",
    "$$\\mathbf{\\frac{d}{dx}(x^n)=nx^{n-1}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KP4jUOaqpP9P"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Правила вычисления проиводной</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fb0go1lpP9Q"
   },
   "source": [
    "Выпишем правила *дифференцирования*:  \n",
    "\n",
    "1). Если $f(x)$ - константа, то её производная (приращение) 0:  \n",
    "\n",
    "$$(C)' = 0$$\n",
    "\n",
    "2). Производная суммы функций - это сумма производных:  \n",
    "\n",
    "$$(f(x) + g(x))' = f'(x) + g'(x)$$\n",
    "\n",
    "3). Производная разности - разность производных:  \n",
    "\n",
    "$$(f(x) - g(x))' = f'(x) - g'(x)$$\n",
    "\n",
    "4). Производная произведения функций:  \n",
    "\n",
    "$$(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)$$\n",
    "\n",
    "5). Производная частного:  \n",
    "\n",
    "$$\\left(\\frac{f(x)}{g(x)}\\right)'=\\frac{f'(x)g(x)-g'(x)f(x)}{g^2(x)}$$\n",
    "\n",
    "6). Производная сложной функции (\"правило цепочки\", \"chain rule\"):  \n",
    "\n",
    "$$(f(g(x)))'=f'(g(x))g'(x)$$\n",
    "\n",
    "Можно записать ещё так:  \n",
    "\n",
    "$$\\frac{d}{dx}(f(g(x)))=\\frac{df}{dg}\\frac{dg}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFTReW5ppP9R"
   },
   "source": [
    "**Примеры**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grjqr2h4pP9R"
   },
   "source": [
    "* Вычислим производную функции $$f(x) = \\frac{x^2}{cos(x)} + 100$$:  \n",
    "\n",
    "$$f'(x) = \\left(\\frac{x^2}{cos(x)}+100\\right)' = \\left(\\frac{x^2}{cos(x)}\\right)' + (100)' = \\frac{(2x)\\cos(x) - x^2(-\\sin(x))}{cos^2(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSqCaSSYpP9T"
   },
   "source": [
    "* Вычислим производную функции $$f(x) = tg(x)$$:  \n",
    "\n",
    "$$f'(x) = \\left(tg(x)\\right)' = \\left(\\frac{\\sin(x)}{\\cos(x)}\\right)' = \\frac{\\cos(x)\\cos(x) - \\sin(x)(-\\sin(x))}{cos^2(x)} = \\frac{1}{cos^2(x)}$$\n",
    "\n",
    "* Проверить решение http://www.wolframalpha.com/input/?i=dirivative+tan(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXP_YETzpP9T"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Частные производные</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJDwZBCZpP9T"
   },
   "source": [
    "Когда мы имеем функци многих переменных, её уже сложнее представить себе в виде рисунка (в случае более 3-х переменных это действительно не всем дано). ОДнако формальные правила взятия производной у таких функций созраняются. Они в точности совпадают с тоеми, которые рассмотрены выше для функции одной переменной.  \n",
    "\n",
    "Итак, правило взятия частной производной функции мнгих переменных:  \n",
    "1). Пусть $f(\\overline{x}) = f(x_1, x_2, .., x_n)$ - функция многих переменных;  \n",
    "2). Частная проиводная по $x_i$ это функции - это производная по x_i, считая все остальные переменные **константами**. \n",
    "\n",
    "Более математично:  \n",
    "\n",
    "Частная производная функции $f(x_1,x_2,...,x_n)$ по $x_i$ равна  \n",
    "\n",
    "$$\\frac{\\partial f(x_1,x_2,...,x_n)}{\\partial x_i}=\\frac{df_{x_1,...,x_{i-1},x_{i+1},...x_n}(x_i)}{dx_i}$$  \n",
    "\n",
    "где $f_{x_1,...,x_{i-1},x_{i+1},...x_n}(x_i)$ означает, что переменные $x_1,...,x_{i-1},x_{i+1},...x_n$ - это фиксированные значения, и с ними нужно обращаться как с константами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3uPIkZ-wpP9U"
   },
   "source": [
    "**Примеры**:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aodpt9VppP9V"
   },
   "source": [
    "* Найдём частные производные функции $f(x, y) = -x^7 + (y - 2)^2 + 140$ по $x$ и по $y$:  \n",
    "\n",
    "$$f_x'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{x}} = -7x^6$$  \n",
    "$$f_y'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = 2(y - 2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3pRitR-YpP9W"
   },
   "source": [
    "* Найдём частные производные функции $f(x, y, z) = \\sin(x)\\cos(y)tg(z)$ по $x$, по $y$ и по $z$:  \n",
    "\n",
    "$$f_x'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{x}} = \\cos(x)\\cos(y)tg(z)$$  \n",
    "$$f_y'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = \\sin(x)(-\\sin(y))tg(z)$$\n",
    "$$f_z'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = \\frac{\\sin(x)\\cos(y)}{\\cos^2{z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrmPlYyrpP9X"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Градиентный спуск</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeD8U4CApP9X"
   },
   "source": [
    "**Градиентом** функции $f(\\overline{x})$, где $\\overline{x} \\in \\mathbb{R^n}$, то есть $\\overline{x} = (x_1, x_2, .., x_n)$, называется вектор из частных производных функции $f(\\overline{x})$:  \n",
    "\n",
    "$$grad(f) = \\nabla f(\\overline{x}) = \\left(\\frac{\\partial{f(\\overline{x})}}{\\partial{x_1}}, \\frac{\\partial{f(\\overline{x})}}{\\partial{x_2}}, .., \\frac{\\partial{f(\\overline{x})}}{\\partial{x_n}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcjYCHOepP9Y"
   },
   "source": [
    "Есть функция $f(x)$. Хотим найти аргумент, при котором она даёт минимум.\n",
    "\n",
    "Алгоритм градиентного спуска:  \n",
    "1. $x^0$ - начальное значение (обычно берётся просто из разумных соображений или случайное);  \n",
    "2. $x^i = x^{i-1} - \\alpha \\nabla f(x^{i-1})$, где $\\nabla f(x^{i-1})$ - это градиент функции $f$, в который подставлено значение $x^{i-1}$;\n",
    "3. Выполнять пункт 2, пока не выполнится условие остановки: $||x^{i} - x^{i-1}|| < eps$, где $||x^{i} - x^{i-1}|| = \\sqrt{(x_1^i - x_1^{i-1})^2 + .. + (x_n^i - x_n^{i-1})^2}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zX1miuQ0pP9Z"
   },
   "source": [
    "**Примеры:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1M6agxdpP9Z"
   },
   "source": [
    "* *Пример 1*: Посчитаем формулу градиентного спуска для функции $f(x) = 10x^2$:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WMJRqDRpP9a"
   },
   "source": [
    "$x^i = x^{i-1} - \\alpha \\nabla f(x^{i-1}) = x^{i-1} - \\alpha f'(x^{i-1}) = x^{i-1} - \\alpha (20x^{i-1}) = x^{i-1} - 20\\alpha x^{i-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqjopRZVpP9b"
   },
   "source": [
    "Имея эту формулу, напишем код градиентного спуска для функции $f(x) = 10x^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "evLahkyIpP9c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def f(x):\n",
    "    return 10 * x**2\n",
    "\n",
    "def gradient_descent(alpha=0.01, eps=0.001):\n",
    "    x_pred = 100  # начальная инициализация\n",
    "    x = 50  # начальная инициализация\n",
    "    for _ in range(1000):\n",
    "        # смотрим, на каком мы шаге, значение x и значение функции\n",
    "        print(\"Step:\", _, \"\\tX =\", round(x,5), \"\\tf(X) =\", round(f(x), 5))  \n",
    "        if (x - x_pred)**2 < eps**2:  # условие остановки\n",
    "            break\n",
    "        x_pred = x\n",
    "        x = x - alpha * (20 * x_pred)  # по формуле выше\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9k8A7ei8pP9g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 \tX = 50 \tf(X) = 25000\n",
      "Step: 1 \tX = 40.0 \tf(X) = 16000.0\n",
      "Step: 2 \tX = 32.0 \tf(X) = 10240.0\n",
      "Step: 3 \tX = 25.6 \tf(X) = 6553.6\n",
      "Step: 4 \tX = 20.48 \tf(X) = 4194.304\n",
      "Step: 5 \tX = 16.384 \tf(X) = 2684.35456\n",
      "Step: 6 \tX = 13.1072 \tf(X) = 1717.98692\n",
      "Step: 7 \tX = 10.48576 \tf(X) = 1099.51163\n",
      "Step: 8 \tX = 8.38861 \tf(X) = 703.68744\n",
      "Step: 9 \tX = 6.71089 \tf(X) = 450.35996\n",
      "Step: 10 \tX = 5.36871 \tf(X) = 288.23038\n",
      "Step: 11 \tX = 4.29497 \tf(X) = 184.46744\n",
      "Step: 12 \tX = 3.43597 \tf(X) = 118.05916\n",
      "Step: 13 \tX = 2.74878 \tf(X) = 75.55786\n",
      "Step: 14 \tX = 2.19902 \tf(X) = 48.35703\n",
      "Step: 15 \tX = 1.75922 \tf(X) = 30.9485\n",
      "Step: 16 \tX = 1.40737 \tf(X) = 19.80704\n",
      "Step: 17 \tX = 1.1259 \tf(X) = 12.67651\n",
      "Step: 18 \tX = 0.90072 \tf(X) = 8.11296\n",
      "Step: 19 \tX = 0.72058 \tf(X) = 5.1923\n",
      "Step: 20 \tX = 0.57646 \tf(X) = 3.32307\n",
      "Step: 21 \tX = 0.46117 \tf(X) = 2.12676\n",
      "Step: 22 \tX = 0.36893 \tf(X) = 1.36113\n",
      "Step: 23 \tX = 0.29515 \tf(X) = 0.87112\n",
      "Step: 24 \tX = 0.23612 \tf(X) = 0.55752\n",
      "Step: 25 \tX = 0.18889 \tf(X) = 0.35681\n",
      "Step: 26 \tX = 0.15112 \tf(X) = 0.22836\n",
      "Step: 27 \tX = 0.12089 \tf(X) = 0.14615\n",
      "Step: 28 \tX = 0.09671 \tf(X) = 0.09354\n",
      "Step: 29 \tX = 0.07737 \tf(X) = 0.05986\n",
      "Step: 30 \tX = 0.0619 \tf(X) = 0.03831\n",
      "Step: 31 \tX = 0.04952 \tf(X) = 0.02452\n",
      "Step: 32 \tX = 0.03961 \tf(X) = 0.01569\n",
      "Step: 33 \tX = 0.03169 \tf(X) = 0.01004\n",
      "Step: 34 \tX = 0.02535 \tf(X) = 0.00643\n",
      "Step: 35 \tX = 0.02028 \tf(X) = 0.00411\n",
      "Step: 36 \tX = 0.01623 \tf(X) = 0.00263\n",
      "Step: 37 \tX = 0.01298 \tf(X) = 0.00168\n",
      "Step: 38 \tX = 0.01038 \tf(X) = 0.00108\n",
      "Step: 39 \tX = 0.00831 \tf(X) = 0.00069\n",
      "Step: 40 \tX = 0.00665 \tf(X) = 0.00044\n",
      "Step: 41 \tX = 0.00532 \tf(X) = 0.00028\n",
      "Step: 42 \tX = 0.00425 \tf(X) = 0.00018\n",
      "Step: 43 \tX = 0.0034 \tf(X) = 0.00012\n"
     ]
    }
   ],
   "source": [
    "x_min = gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGOVAybRpP9k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003402823669209384"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_EW8AY8VpP9n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00011579208923731614"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsqfsTezpP9q"
   },
   "source": [
    "* *Пример 2*: Посчитаем формулу градиентного спуска для функции $f(x, y) = 10x^2 + y^2$:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNWMBcbNpP9r"
   },
   "source": [
    "$$\\left(\\begin{matrix} x^i \\\\ y^i \\end{matrix}\\right) = \\left(\\begin{matrix} x^{i-1} \\\\ y^{i-1} \\end{matrix}\\right) - \\alpha \\nabla f(x^{i-1}, y^{i-1}) = \\left(\\begin{matrix} x^{i-1} \\\\ y^{i-1} \\end{matrix}\\right) - \\alpha \\left(\\begin{matrix} \\frac{\\partial{f(x^{i-1}, y^{i-1})}}{\\partial{x}} \\\\ \\frac{\\partial{f(x^{i-1}, y^{i-1})}}{\\partial{y}} \\end{matrix}\\right) = x^{i-1} - \\alpha \\left(\\begin{matrix} 20x^{i-1} \\\\ 2y^{i-1} \\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBnijsKLpP9r"
   },
   "source": [
    "Осталось написать код, выполняющий градиентный спуск, пока не выполнится условие остановки, для функции $f(x, y) = 10x^2 + y^2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9N7PP5KZQcBU"
   },
   "source": [
    "График\n",
    "https://www.google.com/search?ei=HqS4W7LHKYySrgS73Zdo&q=10x%5E2%2B2y%5E2&oq=10x%5E2%2B2y%5E2&gs_l=psy-ab.3..0i8i7i30k1l9j0i5i30k1.8563.8563.0.11261.1.1.0.0.0.0.102.102.0j1.1.0....0...1c.1.64.psy-ab..0.1.99....0.VOSO7usqihA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "p_rDsja-pP9s"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def f(x):\n",
    "    return 10 * x[0]**2 + x[1]**2\n",
    "\n",
    "def gradient_descent(alpha=0.01, eps=0.001):\n",
    "    x_pred = np.array([100, 100])  # начальная инициализация\n",
    "    x = np.array([50, 50])  # начальная инициализация\n",
    "    for _ in range(100000):\n",
    "        # смотрим, на каком мы шаге, значение x и значение функции\n",
    "        print(\"Step:\", _, \"\\tX =\", np.round(x,3), \"\\tf(X) =\", np.round(f(x), 5))\n",
    "        if np.sum((x - x_pred)**2) < eps**2:  # условие остановки\n",
    "            break\n",
    "        x_pred = x\n",
    "        x = x_pred - alpha * np.array(20 * x_pred[0], 2 * x_pred[1])  # по формуле выше\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "boueQCnXpP9u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 \tX = [50 50] \tf(X) = 27500\n",
      "Step: 1 \tX = [ 40.  40.] \tf(X) = 17600.0\n",
      "Step: 2 \tX = [ 32.  32.] \tf(X) = 11264.0\n",
      "Step: 3 \tX = [ 25.6  25.6] \tf(X) = 7208.96\n",
      "Step: 4 \tX = [ 20.48  20.48] \tf(X) = 4613.7344\n",
      "Step: 5 \tX = [ 16.384  16.384] \tf(X) = 2952.79002\n",
      "Step: 6 \tX = [ 13.107  13.107] \tf(X) = 1889.78561\n",
      "Step: 7 \tX = [ 10.486  10.486] \tf(X) = 1209.46279\n",
      "Step: 8 \tX = [ 8.389  8.389] \tf(X) = 774.05619\n",
      "Step: 9 \tX = [ 6.711  6.711] \tf(X) = 495.39596\n",
      "Step: 10 \tX = [ 5.369  5.369] \tf(X) = 317.05341\n",
      "Step: 11 \tX = [ 4.295  4.295] \tf(X) = 202.91418\n",
      "Step: 12 \tX = [ 3.436  3.436] \tf(X) = 129.86508\n",
      "Step: 13 \tX = [ 2.749  2.749] \tf(X) = 83.11365\n",
      "Step: 14 \tX = [ 2.199  2.199] \tf(X) = 53.19274\n",
      "Step: 15 \tX = [ 1.759  1.759] \tf(X) = 34.04335\n",
      "Step: 16 \tX = [ 1.407  1.407] \tf(X) = 21.78774\n",
      "Step: 17 \tX = [ 1.126  1.126] \tf(X) = 13.94416\n",
      "Step: 18 \tX = [ 0.901  0.901] \tf(X) = 8.92426\n",
      "Step: 19 \tX = [ 0.721  0.721] \tf(X) = 5.71153\n",
      "Step: 20 \tX = [ 0.576  0.576] \tf(X) = 3.65538\n",
      "Step: 21 \tX = [ 0.461  0.461] \tf(X) = 2.33944\n",
      "Step: 22 \tX = [ 0.369  0.369] \tf(X) = 1.49724\n",
      "Step: 23 \tX = [ 0.295  0.295] \tf(X) = 0.95824\n",
      "Step: 24 \tX = [ 0.236  0.236] \tf(X) = 0.61327\n",
      "Step: 25 \tX = [ 0.189  0.189] \tf(X) = 0.39249\n",
      "Step: 26 \tX = [ 0.151  0.151] \tf(X) = 0.2512\n",
      "Step: 27 \tX = [ 0.121  0.121] \tf(X) = 0.16077\n",
      "Step: 28 \tX = [ 0.097  0.097] \tf(X) = 0.10289\n",
      "Step: 29 \tX = [ 0.077  0.077] \tf(X) = 0.06585\n",
      "Step: 30 \tX = [ 0.062  0.062] \tf(X) = 0.04214\n",
      "Step: 31 \tX = [ 0.05  0.05] \tf(X) = 0.02697\n",
      "Step: 32 \tX = [ 0.04  0.04] \tf(X) = 0.01726\n",
      "Step: 33 \tX = [ 0.032  0.032] \tf(X) = 0.01105\n",
      "Step: 34 \tX = [ 0.025  0.025] \tf(X) = 0.00707\n",
      "Step: 35 \tX = [ 0.02  0.02] \tf(X) = 0.00453\n",
      "Step: 36 \tX = [ 0.016  0.016] \tf(X) = 0.0029\n",
      "Step: 37 \tX = [ 0.013  0.013] \tf(X) = 0.00185\n",
      "Step: 38 \tX = [ 0.01  0.01] \tf(X) = 0.00119\n",
      "Step: 39 \tX = [ 0.008  0.008] \tf(X) = 0.00076\n",
      "Step: 40 \tX = [ 0.007  0.007] \tf(X) = 0.00049\n",
      "Step: 41 \tX = [ 0.005  0.005] \tf(X) = 0.00031\n",
      "Step: 42 \tX = [ 0.004  0.004] \tf(X) = 0.0002\n",
      "Step: 43 \tX = [ 0.003  0.003] \tf(X) = 0.00013\n",
      "Step: 44 \tX = [ 0.003  0.003] \tf(X) = 8e-05\n"
     ]
    }
   ],
   "source": [
    "x_min = gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pyhQsmXpP9x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00272226,  0.00272226])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytAfn_X7pP90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.1517630823070564e-05"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyKaCWuJpP93"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Домашнее задание</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrjiC9mUpP93"
   },
   "source": [
    "1). (только для тех, кто раньше брал производные) Вычислите производную функции $f(x)=\\frac{1}{x}$ по определению и сравните с производной степенной функции в общем случае;  \n",
    "2). Найдите производную функции $Cf(x)$, где С - число;  \n",
    "3). Найдите производные функций:  \n",
    "\n",
    "$$f(x)=x^3+3\\sqrt{x}-e^x$$\n",
    "\n",
    "$$f(x)=\\frac{x^2-1}{x^2+1}$$\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "$$L(y, \\hat{y}) = (y-\\hat{y})^2$$  \n",
    "\n",
    "4). Напишите формулу и код для градиентного спуска для функции:  \n",
    "$$f(w, x) = \\frac{1}{1 + e^{-wx}}$$  \n",
    "\n",
    "То есть по аналогии с примером 2 вычислите частные производные по $w$ и по $x$ и запишите формулу векторно (см. пример 2)\n",
    "\n",
    "В задаче 3 производную нужно брать по $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxDBOB04pP93"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Полезные ссылки</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nm0VC825pP95"
   },
   "source": [
    "0). Прикольный сайт с рисунками путём задания кривых уравнениями и функциями:  \n",
    "\n",
    "https://www.desmos.com/calculator/jwshvscdzb\n",
    "\n",
    "***Производные:***\n",
    "\n",
    "1). Про то, как брать частные производные:  \n",
    "\n",
    "http://www.mathprofi.ru/chastnye_proizvodnye_primery.html\n",
    "\n",
    "2). Сайт на английском, но там много видеоуроков и задач по производным:  \n",
    "\n",
    "https://www.khanacademy.org/math/differential-calculus/derivative-intro-dc\n",
    "\n",
    "3). Задачи на частные производные:  \n",
    "\n",
    "http://ru.solverbook.com/primery-reshenij/primery-resheniya-chastnyx-proizvodnyx/  \n",
    "\n",
    "4). Ещё задачи на частные проивзодные:  \n",
    "\n",
    "https://xn--24-6kcaa2awqnc8dd.xn--p1ai/chastnye-proizvodnye-funkcii.html  \n",
    "\n",
    "5). Производные по матрицам:  \n",
    "\n",
    "http://nabatchikov.com/blog/view/matrix_der  \n",
    "\n",
    "***Градиентны спуск:***\n",
    "\n",
    "6). [Основная статья по градиентному спуску](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%81%D0%BF%D1%83%D1%81%D0%BA%D0%B0)\n",
    "\n",
    "7). Статья на Хабре про градиетный спуск для нейросетей:  \n",
    "\n",
    "https://habr.com/post/307312/  \n",
    "\n",
    "***Методы оптимизации в нейронных сетях:***\n",
    "\n",
    "8). Сайт с анимациями того, как сходятся алгоритмы градиентного спуска:\n",
    "www.denizyuret.com/2015/03/alec-radfords-animations-for.html\n",
    "\n",
    "9). Статья на Хабре про метопты (град. спуск) в нейронках:\n",
    "https://habr.com/post/318970/\n",
    "\n",
    "10). Ещё сайт (англ.) про метопты (град. спуск) в нейронках (очень подробно):\n",
    "http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]derivative_gradient.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
